{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analytics\n",
    "\n",
    "## Ingesting Data with Pandas\n",
    "\n",
    "![Python and Pandas!](./images/PythonPandasandDataIngestion.png)\n",
    "\n",
    "## We are going to learn about ...\n",
    "\n",
    "- Reading Data from Excel files\n",
    "- Reading data from SQL databases\n",
    "- Reading data from CSV files\n",
    "- We'll do some Data Wrangling\n",
    "- Pandas in class Practice\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data From Excel into a `DataFrame`:\n",
    "\n",
    "To read an excel file as a DataFrame, use the Python  Pandas `read_excel()` method. You can read different types of Excel file extensions:.xlsx, and .xls.\n",
    "\n",
    "You can read the first sheet, specific sheets, multiple sheets or all sheets. Pandas converts this to the DataFrame structure, which is a tabular like structure.\n",
    "\n",
    "To be able to open Excel files we need to install the module `openpyxl`\n",
    "\n",
    "    --  pip install openpyxl\n",
    "\n",
    "For our example, we will use a file from the resources folder in the curriculum. The filepath to the XLSX file is `./resources/sample_winterathletes.xlsx`. \n",
    "\n",
    "We will use the `read_excel` method as mentioned.\n",
    "\n",
    "-   The first parameter is the name of the excel file.\n",
    "-   The sheet_name parameter defines the sheet to be read from the excel file. By default, Pandas will use the first sheet (positionally), unless otherwise specified. To pass multiple sheets use: - `sheet_name=['East', 'West']`\n",
    "\n",
    "The name of the sheet we want to pull from the Excel workbook is `Athletes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import an excel sheet\n",
    "import pandas as pd\n",
    "\n",
    "athletes = pd.read_excel('./resources/sample_winterathletes.xlsx',\n",
    "                    sheet_name='Athletes')\n",
    "athletes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often you don’t want to load every column in an Excel file because there are too many columns etc. Use the `usecols=` parameter to select the columns of data you want. EX:-- `usecols=['Customer', 'Sales']`\n",
    "\n",
    "Here is a list of some important parameters that can be used with the `.read_excel()` method:-\n",
    "-   **dtype** – Dict with column name an type.\n",
    "-   **nrows** – How many rows to parse.\n",
    "-   **na_values** – Additional strings to recognize as NA/NaN. \n",
    "-   **keep_default_na** – Whether or not to include the default NaN values when parsing the data. \n",
    "-   **na_filter** – Filters missing values.\n",
    "-   **parse_dates** – Specify the column index you wanted to parse as dates\n",
    "-   **thousands** – Thousands separator for parsing string columns to numeric.\n",
    "-   ***skipfooter*** – Specify how many rows you wanted to skip from the footer.\n",
    "-   **mangle_dupe_cols** – Duplicate columns will be specified as ‘X’, ‘X.1’, …’X.N’, \n",
    "\n",
    "The `.read_excel()` method is a very complex function with many parameters that offer a wide range of useful ways to manipulate Excel input for a Pandas DataFrame. Read all about it here in this documantation -- [Syntax for `.read_excel()`.](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html).\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Reading Data From `SQL`  into a `DataFrame`:\n",
    "\n",
    "Pandas has 2 **“read SQL”** methods:-  `pandas.read_sql_query()` and `pandas.read_sql()`. \n",
    "\n",
    "The `.read_sql()` method was added to make it slightly easier to work with SQL data in Pandas. It combines the functionality of two other SQL methods:\n",
    "\n",
    "-   `.read_sql_query()` -- for querying a database and reading the response into a DataFrame once a connection has been setup to the database;\n",
    "-   `.read_sql_table()` -- which allows Pandas to read a whole SQL table from a database into a DataFrame.\n",
    "\n",
    "Syntax of Pandas `.read_sql()`: -\n",
    "\n",
    "```python\n",
    "    # Syntax of read_sql()\n",
    "    pandas.read_sql(sql, con, index_col=None, coerce_float=True, \n",
    "        params=None, parse_dates=None, columns=None, chunksize=None)\n",
    "\n",
    "    # Syntax of read_sql_query()\n",
    "    pandas.read_sql_query(sql, con, index_col=None, coerce_float=True, \n",
    "        params=None, parse_dates=None, chunksize=None, dtype=None)\n",
    "\n",
    "    # Syntax of read_sql_table()\n",
    "    pandas.read_sql_table(table_name, con, schema=None, index_col=None, \n",
    "        coerce_float=True, parse_dates=None, columns=None, \n",
    "        chunksize=None)\n",
    "\n",
    "```\n",
    "\n",
    "There are drivers available for several SQL databases including SQLite, MySQL, PostgreSQL, etc.\n",
    "\n",
    "Python comes with build-in support for SQLite. However, unlike SQLite, there are no built-in Python SQL libraries for connecting to other databases. \n",
    "\n",
    "In order to connect with other databases from within a Python program, we'll need to install SQL drivers for Python, for those desired databases, and then define connectors for those drivers. A complex process.\n",
    "\n",
    "We wil use the `pandas.read_sql_query()` as mentioned, along with the `.sqlite` SQL module.\n",
    "\n",
    "The filepath is `./resources/pitchforkDatabase.sqlite`. \n",
    "\n",
    "Use the `sqlite3` library. The name of the table is `artists`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "con = sqlite3.connect('./resources/PitchForkDatabase.sqlite')\n",
    "pfDB = pd.read_sql_query('select * from artists',con)\n",
    "pfDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Pandas handles importing SQL data way more eligantly than Python does, the `.read_sql_query()` method is still a complex function with many parameters that offer a wide range of ways to manipulate SQL input to form a Pandas DataFrame. \n",
    "\n",
    "Read all about the [Syntax and use of `.read_sql()`.](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html)\n",
    "\n",
    "And here is a very simple and basic article explaining the most useful minimums for working with SQL in Pandas; simple and easy -- [pandas read_sql() method implementation with Examples](https://www.datasciencelearner.com/pandas-read_sql-implementation-examples/)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Reading data from `CSV files` into a `DataFrame`:\n",
    "\n",
    "A simple way to store big data sets is to use CSV files (comma separated files).\n",
    "\n",
    "CSV files contains plain text and is a well know format that can be read by everyone including Pandas.\n",
    "\n",
    "You can open it in Notepad but the format will be off; Use VS Code instead.\n",
    "\n",
    "To access data from the CSV file, we require a function `.read_csv()` that retrieves data in the form of the DataFrame.\n",
    "\n",
    "By default, a `CSV` is separated by commas. But one can use other separators as well. \n",
    "\n",
    "The `pandas.read_csv()` function is not limited to reading the CSV file with default separator (i.e. comma). It can be used for other separators such as `;` or `|` or `:`. \n",
    "\n",
    "To load CSV files with such separators, the `sep=` parameter is used to pass the separator used in the CSV file. Example: --\n",
    "```python\n",
    "    f = pd.read_csv(\"datafile2.csv\", sep='|')\n",
    "```\n",
    "\n",
    "For our example, we'll use a file from the resources folder in the curriculum. The filepath to the CSV file is `./resources/GREENCOMPUTERS500.csv`.\n",
    "\n",
    "Lets first look at the data by clicking this link ... [Top 500 Green Computers](./resources/GREENCOMPUTERS500.csv)\n",
    "\n",
    "Form this data we can see that we have a file with many columns.\n",
    "\n",
    "Lets see what it loos like when we import the data into a DataFrame ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing a CSV file\n",
    "import pandas as pd\n",
    "\n",
    "green = pd.read_csv('./resources/GREENCOMPUTERS500.csv',index_col=0)\n",
    "green.info()\n",
    "green"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all about the [Syntax and use of `.read_csv()`.](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Creating a `.csv` file from scratch\n",
    "\n",
    "We can export a Pandas DataFrame to a CSV file by using the Pandas `to_csv()` method. By default, this method exports a DataFrame to a CSV file with \"row index\" as the first column, and a comma as the delimiter. \n",
    "\n",
    "With the `sep:` we can specify a custom delimiter for the CSV output, instead of a comma.\n",
    "\n",
    "One can also save the CSV with out indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a .csv from scratch\n",
    "import pandas as pd \n",
    "\n",
    "cities = pd.DataFrame([[\"St. Louis\", \"Missouri\"], [\"Atlanta\", \"Georgia\"]], \n",
    "                        columns=[\"City\", \"State\"])\n",
    "cities\n",
    "cities.to_csv('cities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sep: Specify a custom delimiter for the CSV output\n",
    "import pandas as pd \n",
    "\n",
    "cities = pd.DataFrame([[\"St. Louis\", \"Missouri\"], [\"Atlanta\", \"Georgia\"]], \n",
    "                        columns=[\"City\", \"State\"])\n",
    "\n",
    "cities.to_csv('citiesT.csv', sep='\\t')   # use tab to separate data instead of a comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the newly created  .csv file\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('cities.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the file without indexes\n",
    "import pandas as pd \n",
    "\n",
    "cities = pd.DataFrame([[\"St. Louis\", \"Missouri\"], [\"Atlanta\", \"Georgia\"]],\n",
    "                        columns=[\"City\", \"State\"])\n",
    "\n",
    "cities.to_csv('cities.csv', index=False)\n",
    "df = pd.read_csv('cities.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all about how to use [pandas.DataFrame.to_csv](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Pandas Data Wrangling with a CSV file\n",
    "\n",
    "We will reuse the data file we introduced in the 1st Pandas session. For our example, we will use a file from the resources folder in the curriculum. The filepath to the CSV file is `./resources/data.csv`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and print a summary of a DataFrame\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./resources/data.csv\")\n",
    "\n",
    "# Print Summary of a DataFrame -- 1st 5 & last 5 lines\n",
    "print(\"Summary of our DataFrame ...\\n\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and print N number of rows from a DataFrame\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./resources/data.csv\")\n",
    "\n",
    "# Viewing the FIRST 10 rows\n",
    "print(\"\\nFirst 10 header rows ...\\n\\n\", df.head(10))\n",
    "\n",
    "# Viewing the LAST 12 rows\n",
    "print(\"\\nLast 12 tailing rows ...\\n\\n\", df.tail(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and print Info about a DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./resources/data.csv\")\n",
    "\n",
    "# Information about the DataFrame\n",
    "print(\"\\nPrint Info on the DataFrame ...\\n\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A closer look at the DataFrame Info …\n",
    "\n",
    "![DataFrame Info Display](./images/Pandas_DF_InfoDisplay.png)\n",
    "\n",
    "Looking at this Info reveals that there are 5 rows in the 'Calories' column without data.\n",
    "\n",
    "Nulls are bad! Nulls are the wrong result when you analyze data.\n",
    "\n",
    "### Let’s find the Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find the NULLS ...\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./resources/data.csv\")\n",
    "\n",
    "# This statement gives access to the WHOLE DataFrame\n",
    "print(\"\\nDF print ...\\n\", df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dropping the NULLs -- `.dropna()`**\n",
    "\n",
    "If you studied the output you should have found that the following lines contain NULLs in the 'Calories' column: 17, 27, 91, 118, 141\n",
    "\n",
    "We will use the `.dropna()` method to drop the NULLs.\n",
    "\n",
    "The `dropna()` method removes the rows that contains NULL values.\n",
    "\n",
    "The `dropna()` method returns a new DataFrame object unless the `INPLACE` parameter is set to `True`, in that case the `dropna()` method does the removing in the original DataFrame instead.\n",
    "\n",
    "> Note: In the example below, this DOES NOT change the original DataFrame BECAUSE we are using `new_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping NULS -- .dropna()\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./resources/data.csv\")\n",
    "\n",
    "new_df = df.dropna()\n",
    "# check columns: 17\t27\t91\t118\t141\n",
    "print(new_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This changes the ORIGINAL DataFrame\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./resources/data.csv\")\n",
    "\n",
    "df.dropna(inplace = True)\n",
    "# check columns: 17\t27\t91\t118\t141\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about the `.dropna()` method ... [pandas.DataFrame.dropna](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Replacing Nulls -- `.fillna()`**\n",
    "\n",
    "Often for data integrity sake we do not want to drop NULLs but transform and preserve them. We can replace NULLS with other values, or even calculations.\n",
    "\n",
    "The `fillna()` method replaces the NULL values with a specified value.\n",
    "\n",
    "The `fillna()` method returns a new DataFrame object unless the `inplace` parameter is set to True, in that case the `fillna()` method does the replacing in the original DataFrame instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAUTION: -- Replace **ALL** NULLs with 130\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./resources/data.csv\")\n",
    "\n",
    "df.fillna(130, inplace = True)\n",
    "# check columns: 17\t27\t91\t118\t141\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing Nulls in the \"Calories\" column with 130\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./resources/data.csv\")\n",
    "\n",
    "df[\"Calories\"].fillna(130, inplace = True)\n",
    "# check columns: 17\t27\t91\t118\t141\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about the `.fillna()` method ... [pandas.DataFrame.fillna](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Replace Nulls using Mean, Median, Mode**\n",
    "\n",
    "We can replace NULLS with specific calculations to \"fill in\" missing data with valid similar data.\n",
    "\n",
    "A common way to replace empty cells, is to calculate the Mean, Median or Mode value of the column.\n",
    "\n",
    "Pandas uses the `mean()`, `median()` and `mode()` methods to calculate the respective values for a specified column.\n",
    "\n",
    "- **Mean** = the average value\n",
    "- **Median** = the value in the middle, after you have sorted all the values ascending\n",
    "- **Mode** = the value that appears most frequently\n",
    "\n",
    "One of the key points is to decide which technique, out of the above-mentioned imputation techniques, to use for getting the most appropriate approximation for the missing values.\n",
    "\n",
    ">The goal is to find out which is a better measure of the central tendency of data and use that value for replacing missing values appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NULLS with MEAN\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./resources/data.csv\")\n",
    "\n",
    "x = round((df[\"Calories\"].mean()), 2)    # the average value\n",
    "# x = round(x, 2)\n",
    "print(\"The mean ...\", x)\n",
    "\n",
    "df[\"Calories\"].fillna(x, inplace = True)\n",
    "# check columns: 17\t27\t91\t118\t141\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NULLS with MEDIAN\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./resources/data.csv\")\n",
    "\n",
    "x = df[\"Calories\"].median()     # the value in the middle\n",
    "print(\"the median ...\", x)\n",
    "\n",
    "df[\"Calories\"].fillna(x, inplace =True)\n",
    "# check columns: 17\t27\t91\t118\t141\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NULLS with MODE\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./resources/data.csv\")\n",
    "\n",
    "x = df[\"Calories\"].mode()[0]    # value that appears most\n",
    "print(\"the mode ...\", x)\n",
    "\n",
    "df[\"Calories\"].fillna(x, inplace = True)\n",
    "# check columns: 17\t27\t91\t118\t141\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### **Replacing Incorrect data in DataFrames**\n",
    "\n",
    "So, you want to replace values in your DataFrame with something else? No problem. That is where Pandas Replace comes in.\n",
    "\n",
    "Pandas `DataFrame.replace()` is a small but powerful function that will replace (or swap) values in your DataFrame with another value. It can replace strings, regex, lists, dictionaries, series, numbers, etc. from a Dataframe. \n",
    "\n",
    "Every instance of the provided value is replaced after a thorough search of the full DataFrame, depending on the parameters used...\n",
    "\n",
    "    Syntax:     df.replace(to_replace = 'what you want to replace',\n",
    "                            value = 'what you want to replace it with')\n",
    "\n",
    "What starts as a simple function, can quickly be expanded for most of your scenarios.\n",
    "\n",
    "Pandas `.replace()` can quickly get nuanced as you dig deeper. Here are the most common ways to use pandas replace:\n",
    "\n",
    "**Pandas Replace**\n",
    "\n",
    "| Code | Plain Language |\n",
    "|----|----|\n",
    "| df.replace(0, 5) | Replace all of the 0s in your DataFrame with 5s |\n",
    "| df.replace([0, 1, 2, 3], 4) | Replace all the 0s, 1s, 2s, 3s in your DataFrame with 4s |\n",
    "| df.replace([0, 1, 2, 3], [4, 3, 2, 1]) | Replace all the 0s with 4s, 1s with 3s, 2s with 2s, and 3s with 1s. Note: if you pass two lists they both much be the same length |\n",
    "| df.replace({0: 10, 1: 100}) | Using a dict – Replace 0s with 10s, and 1s with 100s. |\n",
    "| df.replace({'A': 0, 'B': 5}, 100) | Replace 0’s in column “A” with 100, and replace 5s in column “B” with 100 |\n",
    "| df.replace({'C': {1: 100, 3: 300}}) | Using a dict – Within column “C” replace 1s with 100 and 3s with 300 |\n",
    "| df.replace(to_replace=r'^ba.$', value='new', regex=True) | Replace anything that matched the regex ‘^ba.$’ with “new” |\n",
    "\n",
    "<br>\n",
    "\n",
    "**Replace Parameters**\n",
    "\n",
    "- **to_replace**: - The values, list of values, or values which match regex, you want to replace. If using a dict, you can also include the values you would like to do the replacing\n",
    "- **value**: - Values that will do the replacing. **Note**: This can also be none if you have a `dict` in your `to_replace` parameter\n",
    "- **inplace (Default: False)**: - If true, it write over your current DataFrame. If false, then your DataFrame will be returned to you.\n",
    "- **limit**: - The max size you could like to forward or back fill. Example: The number of rows to fill before and after the current point\n",
    "- **regex**: - If you want `to_replace` to read your inputs as regex or not\n",
    "- **method**: - The fill method to use when `to_replace` is either a scalar, list, or tuple. Value must be None\n",
    "- **pad/ffill**: – Take the value that is in the back of what your replacing, and fill it going forward\n",
    "- **bfill**: – Take the value that is in the front of your value to be replaced, and fill it going backward\n",
    "\n",
    ">`(to_replace)` can be a list consisting of str, regex or numeric objects. If both `to_replace` and `value` are lists, they must be the same length.\n",
    "\n",
    "For full detail on how to use all these options please refer to the documentation on [pandas.DataFrame.replace](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html)\n",
    "\n",
    "\n",
    "**So. let's replace some values ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'X': [1, 2, 3, 4, 5],\n",
    "                    'Y': [5, 6, 7, 8, 9],\n",
    "                    'Z': ['z', 'y', 'x', 'w', 'v']})\n",
    "\n",
    "# Replace all 2s with 20s\n",
    "df1 = df.replace(to_replace=2, value=20)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'X': [1, 2, 3, 4, 5],\n",
    "                    'Y': [5, 6, 7, 8, 9],\n",
    "                    'Z': ['z', 'y', 'x', 'w', 'v']})\n",
    "\n",
    "# replace all 1s, 3s, and 5s with 20\n",
    "df2 = df.replace(to_replace=[1,3,5], value=20)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'X': [1, 2, 3, 4, 5],\n",
    "                    'Y': [5, 6, 7, 8, 9],\n",
    "                    'Z': ['z', 'y', 'x', 'w', 'v']})\n",
    "\n",
    "# here 1s get replaced with 10s, 3s with 30s and 5s with 50s\n",
    "df3 = df.replace(to_replace=[1,3,5], value=[10,30,50])\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'X': [1, 2, 3, 4, 5],\n",
    "                    'Y': [5, 6, 7, 8, 9],\n",
    "                    'Z': ['z', 'y', 'x', 'w', 'v']})\n",
    "\n",
    "# replacing 1s with 10s, 'z's with 'zz's, and 'v's with 'vvv's\n",
    "df4 = df.replace(to_replace={1: 10, 'z':'zz', 'v':'vvv'})\n",
    "print(df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### **Fixing Dates in DataFrames**\n",
    "\n",
    "In most of the big data scenarios , there will be a requirement to fix date issues. It could be necessary to flip a date format, change date formats, or to correct them based on certain region, flag incorrect dates, and fix them appropriately.\n",
    "\n",
    "For the next examples we will use a much smaller dataset from the resources folder.\n",
    "\n",
    "The filepath to the CSV file is `./resources/data1.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing Dates in DataFrames\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./resources/data1.csv\")\n",
    "print(df.to_string())\n",
    "\n",
    "# drop NULL dates inplace\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df.dropna(subset=['Date'], inplace = True)\n",
    "\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    " \n",
    " #### **Fixing wrong info in a specific LOCATION**\n",
    "\n",
    "Pandas provides two ways, `loc()` and `at()`, to access or change a single value of a DataFrame.\n",
    "\n",
    "- Use `at()` if you only need to get or set a single value in a DataFrame or Series.\n",
    "- On the other hand `loc()`can be used to access a single value but also to access a group of rows and columns by a label or labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing wrong info in a specific LOCATION\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./resources/data1.csv\")\n",
    "print(df.to_string())\n",
    "\n",
    "# Change line 9 \"Duration\" from 60 to be 45\n",
    "df.loc[9, 'Duration'] = 45\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    " \n",
    " #### **Fixing wrong info in LARGE sets by looping**\n",
    "\n",
    "\"Wrong data\" does not have to be \"empty cells\" or \"wrong format\", it can just be wrong, like if someone registered \"199\" instead of \"1.99\".\n",
    "\n",
    "Loop through all values in the \"Duration\" column; If the value is higher than 120, set it to 120:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing wrong info in LARGE sets by looping\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./resources/data.csv\")\n",
    "\n",
    "for x in df.index:\n",
    "    if df.loc[x, \"Duration\"] > 120:\n",
    "        df.loc[x, \"Duration\"] = 120\n",
    "\n",
    "# print(df.to_string())\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### **Dropping / Deleting rows from a DataFrame**\n",
    "\n",
    "Another way of handling wrong data is to remove the rows that contains wrong data.\n",
    "\n",
    "This way you do not have to find out what to replace them with, and there is a good chance you do not need them to do your analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing rows in LARGE sets\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./resources/data.csv\")\n",
    "print(df.info())\n",
    "\n",
    "for x in df.index:\n",
    "    if df.loc[x, \"Duration\"] > 120:\n",
    "        df.drop(x, inplace = True)\n",
    "\n",
    "print(df.info())\n",
    "# print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " --- \n",
    " \n",
    " #### **Discovering Duplicates**\n",
    "\n",
    "Duplicate rows are rows that have been registered more than one time.\n",
    "To discover duplicates, we can use the `duplicated()` method.\n",
    "\n",
    "The `duplicated()` method returns a Boolean values for each row; in other words, it returns `True` for every row that is a duplicate, otherwise `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding all Duplicates\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./resources/data1.csv\")\n",
    "print(df.duplicated())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Removing Duplicates**\n",
    "\n",
    "To remove duplicates, use the `drop_duplicates()` method.\n",
    "\n",
    ">Remember: The `inplace = True` will make sure that the method does NOT return a new DataFrame, but it will remove all duplicates from the original DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all Duplicates\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./resources/data1.csv\")\n",
    "\n",
    "df.drop_duplicates(inplace = True)\n",
    "print(df.duplicated())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### More practice with CSV files - Titanic\n",
    "\n",
    "First, we need to gather our data.\n",
    "\n",
    "We can either use the data from our resources directory, or we can import our data from the WEB.\n",
    "\n",
    "The filepath to the CSV file in the curriculum resources folder is [\"./resources/titanic.csv\"](./resources/titanic.csv).\n",
    "\n",
    "Else, the URL to the data file on the WEB is ... https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\n",
    "\n",
    "You can download that file to your machine, or we will pull that file directly in our code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need to gather our data\n",
    "import pandas as pd\n",
    "\n",
    "titanic_data  = pd.read_csv(\"./resources/titanic.csv\")\n",
    "\n",
    "print(titanic_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you wanted to use the data straight from the web\n",
    "import pandas as pd \n",
    "\n",
    "titanic_data = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
    "\n",
    "print(titanic_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you copied the file to your desktop\n",
    "import pandas as pd\n",
    "\n",
    "titanic_data = pd.read_csv(r\"C:\\Users\\User\\Desktop\\DAP2022\\titanic.csv\")\n",
    "\n",
    "print(titanic_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customizing Data Headers\n",
    "import pandas as pd \n",
    "\n",
    "col_names = [\"Id\", \"Survived\", \n",
    "                \"Passenger Class\", \"Full Name\", \n",
    "                \"Gender\", \"Age\", \"SibSp\", \"Parch\", \n",
    "                \"Ticket Number\", \"Price\", \"Cabin\", \"Station\"] \n",
    "\n",
    "titanic_data = pd.read_csv(r\"./resources/titanic.csv\", names = col_names) \n",
    "print(titanic_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skipping Rows\n",
    "import pandas as pd \n",
    "\n",
    "col_names = [\"Id\", \"Survived\", \n",
    "                \"Passenger Class\", \"Full Name\", \n",
    "                \"Gender\", \"Age\", \"SibSp\", \"Parch\", \n",
    "                \"Ticket Number\", \"Price\", \"Cabin\", \"Station\"] \n",
    "\n",
    "titanic_data = pd.read_csv(r\"./resources/titanic.csv\", names = col_names, skiprows=[0]) \n",
    "print(titanic_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving to a new .csv file\n",
    "import pandas as pd \n",
    "\n",
    "col_names = [\"Id\", \"Survived\", \n",
    "                \"Passenger Class\", \"Full Name\", \n",
    "                \"Gender\", \"Age\", \"SibSp\", \"Parch\", \n",
    "                \"Ticket Number\", \"Price\", \"Cabin\", \"Station\"] \n",
    "\n",
    "titanic_data = pd.read_csv(r\"./resources/titanic.csv\", names = col_names, skiprows=[0]) \n",
    "\n",
    "titanic_data.to_csv('use_titanic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the newly created  .csv file\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('use_titanic.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Plotting & Practice with Pandas, Matplotlib and `.csv` files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting directly from a .csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"./resources/data1.csv\")\n",
    "df.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a scatter plot\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"./resources/data1.csv\")\n",
    "\n",
    "df.plot(kind = \"scatter\", x = \"Duration\", y = \"Calories\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Histogram plot\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"./resources/data1.csv\")\n",
    "\n",
    "df[\"Duration\"].plot(kind ='hist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continue working with Matplotlib in the next session"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
