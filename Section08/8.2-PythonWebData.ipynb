{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfbdd9c7",
   "metadata": {},
   "source": [
    "# Data Analytics\n",
    "\n",
    "## Python and Web Data Gathering\n",
    "\n",
    "What we'll look at...\n",
    "- Python and working with the Internet\n",
    "    -   Using HTTP with `urllib`\n",
    "- Data on the Web\n",
    "    -   Using XML\n",
    "    -   Using JSON\n",
    "- BeautifulSoup and Web Scraping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadee5c0",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Python and working with the Internet\n",
    "\n",
    "Communications have come a looong way since someone first had the notion of sending information from point \"A\" to point \"B\". \n",
    "\n",
    "#### Brief History of long distance communications ...\n",
    "\n",
    "- **The Telegraph** -- Early constructions of the telegraph started popping up across the world during the 19th century.\n",
    "\n",
    "- **The Telephone** -- Patented in 1875 by Alexander Graham Bell, the telephone really took off with the invention of the transistor, and electronic switching systems created to connect two parties. The telephone combined the lossless communication of a conversation with the instantaneousness of the telegraph.\n",
    "\n",
    "- **The Radio** -- Radio waves, also known as Hertzian waves, were first discovered in the late 19th century and were later used for commercial purposes by Guglielmo Marconi in 1896. This led to the development of all sorts of wave technology communications, including microwave signals.\n",
    "\n",
    "- **Computer Networking** -- In October of 1969, the first data traveled between nodes of the ARPANET, a predecessor of the Internet. This was the first computer network and was invented by Charley Kline and Bill Duvall. \n",
    "\n",
    "- **The Internet** -- On January 1, 1983, the Internet was officially born. ARPANET switched its old `network control protocols` (NCP) and the new `Transmission Control Protocol/Internet Protocol` (TCP/IP) became standard. Today the internet allows instant communication, between parties on the other side of the planet, using any number of different devices to do that.\n",
    "\n",
    "\n",
    "#### Some basic theory...\n",
    "\n",
    "**TCP Connections / Sockets**\n",
    "-   The `Transport Control Protocol (TCP)` is build on top of `IP (Internat Protocol)`\n",
    "-   TCP handles “flow control” using a transmit window and provides a nice reliable pipe\n",
    "-   A `port` is an application-specific or process-specific software communications endpoint\n",
    "-   Ports allow multiple networked applications to coexist on the same server\n",
    "-   There is a list of well-known TCP port numbers EG:- HTTP is on Port 80\n",
    "-   Sometimes we see the port number in the URL if the web server is running on a “non-standard” port EG:- `www.lasi-asia.org:8080/wp/`\n",
    "-   Python has built-in support for TCP Sockets using the `socket` library\n",
    "\n",
    "<br>\n",
    "\n",
    "![Network Communication Layers](./images/NetworkCmmunicationLayers.jpg)\n",
    "\n",
    "<br>\n",
    "\n",
    "**HTTP - Hypertext Transfer Protocol**\n",
    "-   HTTP is the set of rules to allow browsers to retrieve web documents from servers over the Internet\n",
    "-   HTTP is the dominant Application Layer Protocol on the Internet\n",
    "-   It was invented for the Web - to Retrieve HTML, Images, Documents, etc.\n",
    "-   HTML has extended to retrieve data in addition to documents - RSS, Web Services, etc.\n",
    "-   The basic concept of HTTP - Make a Connection - Request a document - Retrieve the Document - Close the Connection\n",
    "-   When users click on an `href` tag, the browser makes a connection to the web server and issues a “GET” request - to GET the content of that page at the specified URL\n",
    "-   The server returns the HTML document to the browser, which formats and displays the document to the user\n",
    "-   The Python module `socket` elegantly handles HTTP requests in Python\n",
    "\n",
    "<br>\n",
    "\n",
    "**About Characters and Strings…**\n",
    "-   To represent the wide range of characters and character sets computers must be able to handle, we represent characters with more than one byte of data\n",
    "-   `UTF-8` is recommended practice for encoding data to be exchanged between systems. `UTF-8` uses 1-4 bytes of data to represent each character of data\n",
    "-   Inside Python 3, all strings are `Unicode encoded`\n",
    "-   In Python, working with string variables in programs, and reading data from files, usually \"just works\" because of the power of build in methods and functions\n",
    "-   When we talk to a network resource using sockets, or talk to a database, we have to `encode` and `decode` data (usually to UTF-8)\n",
    "\n",
    "<br>\n",
    "\n",
    "**Making HTTP Easier With `urllib`**\n",
    "-   Since HTTP is so common, we have a Python library that does all the socket work for us and makes web pages look like a file\n",
    "\n",
    "-   To use it ... `import urllib.request, urllib.parse, urllib.error`\n",
    "\n",
    "-   Then create a file handler ... `fhand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e302a1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import & read text from a Web Page using HTTP\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "fhand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "for line in fhand:\n",
    "    print(line.decode().strip())    # strip leading and trailing blanks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77225d4",
   "metadata": {},
   "source": [
    "#### Read all about urlLib documentation at ... [HOW TO Fetch Internet Resources Using The urllib Package](https://docs.python.org/3/howto/urllib2.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7f8443",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    " ### Data on the Web\n",
    "\n",
    "- With HTTP Request/Response established, there was a natural move toward exchanging data between programs using this protocol\n",
    "- There are two commonly used formats for sending Data Across the “Net” a.k.a. “Wire Protocols” - What we send on the “wire”\n",
    "- The two commonly formats are: `XML` and `JSON`\n",
    "\n",
    "#### the XML Protocol\n",
    "\n",
    "Agreeing on a “Wire Format” lead to `XML - eXtensible Markup Language`\n",
    "\n",
    "It started as a simplified subset of the Standard Generalized Markup Language (SGML), and is designed to be a relatively human-legible way for information systems to share structured data\n",
    "\n",
    "XML Terminology\n",
    "-   **Tags** - indicate the beginning and ending of elements\n",
    "-   **Attributes** - Keyword/value pairs on the opening tag of XML\n",
    "-   **Serialize / De-Serialize** - Convert data in one program into a common format that can be stored and/or transmitted between systems in a programming language-independent manner\n",
    "-   **XML Schema** - Describing a “contract” as to what is the acceptable legal format of an XML document\n",
    "-   **Schema Languages** - Many XML Schema Languages; the most common \"standard\" is XML Schema from W3C - **XSD** or **“W3C Schema”**\n",
    "-   **“W3C Schema”** - defines a structure, constraints, data types, and syntax for XML transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab8da1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic example of using XML data in Python\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "input = '''\n",
    "    <stuff>\n",
    "        <users>\n",
    "            <user x=\"2\">\n",
    "                <id>001</id>\n",
    "                <name>Chuck</name>\n",
    "            </user>\n",
    "            <user x=\"7\">\n",
    "                <id>009</id>\n",
    "                <name>Brent</name>\n",
    "            </user>\n",
    "        </users>\n",
    "    </stuff>'''\n",
    "\n",
    "stuff = ET.fromstring(input)\n",
    "lst = stuff.findall('users/user')\n",
    "print('User count:', len(lst))\n",
    "\n",
    "for item in lst:\n",
    "    print('Name', item.find('name').text)\n",
    "    print('Id', item.find('id').text)\n",
    "    print('Attribute', item.get(\"x\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09c22c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex example of pulling XML from the internet\n",
    "import xml.etree.ElementTree as ET\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import datetime as dt\n",
    "\n",
    "# pull a list of winning PowerBall numbers since 2010\n",
    "url = 'https://data.ny.gov/api/views/d6yy-54nr/rows.xml?accessType=DOWNLOAD'\n",
    "response = urllib.request.urlopen(url).read()\n",
    "\n",
    "# parse the XML response to build a tree\n",
    "tree = ET.fromstring(response)\n",
    "listLookup = tree.findall('row/row')\n",
    "print('Record count:', len(listLookup))\n",
    "\n",
    "for item in listLookup:\n",
    "    drawDate = item.find('draw_date').text\n",
    "    winningNumber = item.find('winning_numbers').text\n",
    "\n",
    "    # reformat the date to a prettier format\n",
    "    isoDate = dt.datetime.fromisoformat(drawDate)\n",
    "    formatDate = isoDate.strftime(\"%A %d. %B %Y\")\n",
    "\n",
    "    print('\\nDrawing Date: - ', formatDate)\n",
    "    print('Winning Numbers: - ', winningNumber)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fd7c29",
   "metadata": {},
   "source": [
    "#### Read all about the Python xml.etree library documentation at ... [The ElementTree XML](https://docs.python.org/3/library/xml.etree.elementtree.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f73376",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Using the JSON Protocol\n",
    "\n",
    "There is a huge amount of data available on the web and most of it is in (JavaScript Object Notation) JSON.\n",
    "\n",
    "JSON is a lightweight data format for data interchange which can be easily read and written by humans, and easily parsed and generated by machines. It is a complete language-independent text format. \n",
    "\n",
    "Douglas Crockford “Discovered” JSON. JSON represents data as nested “lists” and “dictionaries”in Python.\n",
    "\n",
    "The syntax of JSON is considered as a subset of the syntax of JavaScript including the following:\n",
    "\n",
    "-   **Key Name/Value pairs**: -- Represent Data, name is followed by a `:` (colon) and the Name/Value pairs separated by commas\n",
    "-   **Curly braces**: -- Hold objects\n",
    "-   **Square brackets**: -- Hold arrays with values separated by commas\n",
    "\n",
    "The **Keys/Name** must be a string with double quotes, and **Values** must be data types amongst the following:- String, Number, Object (JSON object), array, Boolean, or Null\n",
    " \n",
    "To make it easier for humans to directly read and use JSON, we have different libraries which help us to read the JSON data fetched from the web. \n",
    "\n",
    "These libraries -- like `json` -- have objects and functions which help to open the URL from the web and read the data.\n",
    "\n",
    "In this way, one can easily read a JSON response from a given URL by using `urlopen()` method to get the response, and then use `json.loads()` to convert the response into a JSON object.\n",
    "\n",
    "Here are the steps of the process by which we can read the JSON response from a link or URL in Python:\n",
    "-   Import required modules\n",
    "-   Assign URL\n",
    "-   Get the response of the URL using `urlopen()`\n",
    "-   Convert it to a JSON response using `json.loads()`\n",
    "-   Display the generated JSON response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e02ccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of JSON data with Python\n",
    "import json\n",
    "\n",
    "input = '''\n",
    "[\n",
    "    { \"id\" : \"001\",\n",
    "        \"x\" : \"2\",\n",
    "        \"name\" : \"Chuck\"\n",
    "    } ,\n",
    "    { \"id\" : \"009\",\n",
    "        \"x\" : \"7\",\n",
    "        \"name\" : \"Chuck\"\n",
    "    }\n",
    "]'''\n",
    "\n",
    "info = json.loads(input)\n",
    "print('User count:', len(info))\n",
    "\n",
    "for item in info:\n",
    "    print(\"\\n\")\n",
    "    print('Name', item['name'])\n",
    "    print('Id', item['id'])\n",
    "    print('Attribute', item['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43530703",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here is an example of pulling JSON from the web\n",
    "# import urllib library and json\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "\n",
    "# store the URL to import in ourURL as parameter for urlopen\n",
    "ourURL = \"https://jsonplaceholder.typicode.com/users\"\n",
    "\n",
    "# store the response of URL\n",
    "response = urlopen(ourURL)\n",
    "\n",
    "# storing the JSON response from url in data\n",
    "data_json = json.loads(response.read())\n",
    "\n",
    "# print the unformatted JSON response data\n",
    "print(\"\\nUnformatted JSON data ...\\n\", data_json)\n",
    "\n",
    "# transform the raw JSON into a structured object\n",
    "data_object = json.dumps(data_json, indent = 4)\n",
    "\n",
    "# print the formatted JSON Object\n",
    "print(\"\\nFormatted JSON Object ...\\n\", data_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d90e89e",
   "metadata": {},
   "source": [
    "Here are some more URLs from JSONPlaceholder data which comes with a set of 6 common resources:\n",
    "\n",
    "-   /posts  --\t100 posts\t--\thttps://jsonplaceholder.typicode.com/posts\n",
    "-   /comments   --\t500 comments --\t https://jsonplaceholder.typicode.com/comments\n",
    "-   /albums --\t100 albums -- https://jsonplaceholder.typicode.com/albums\n",
    "-   /photos --\t5000 photos -- https://jsonplaceholder.typicode.com/photos\n",
    "-   /todos  --\t200 todos -- https://jsonplaceholder.typicode.com/todos\n",
    "-   /users  --\t10 users -- https://jsonplaceholder.typicode.com/users\n",
    "\n",
    "You are welcome to try these out for yourself to see what comes back in these JSON data loads.\n",
    "\n",
    "\n",
    "#### Read all about Python JSON in this article ... [Working With JSON Data in Python](https://realpython.com/python-json/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c63dfb6",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### BeautifulSoup and Web Scraping\n",
    "\n",
    "The incredible amount of data on the Internet is a rich resource for any field of research or personal interest.\n",
    "\n",
    "To effectively harvest that data, you’ll need to become skilled at web scraping.\n",
    "\n",
    "#### What is Web Scraping\n",
    "\n",
    "When a program or script pretends to be a browser and retrieves web pages, looks at those web pages, extracts information, and then looks at more web pages\n",
    "\n",
    "Search engines scrape web pages - we call this “spidering the web” or “web crawling”\n",
    "\n",
    "> Note : Web Scraping is considered as illegal in many cases. It may also cause your IP to be blocked permanently by a website.\n",
    "\n",
    "Some websites don’t like it when automatic scrapers gather their data, while others don’t mind. \n",
    "\n",
    "If you’re scraping a page respectfully for educational purposes, then you’re unlikely to have any problems. Still, it’s a good idea to do some research and make sure that you’re not violating any 'Terms of Service' before you start.\n",
    "\n",
    "#### Scraping Web Pages\n",
    "\n",
    "The Python libraries `requests`, `html5lib` and `BeautifulSoup` are powerful tools, perfect for the job of webs craping.\n",
    "\n",
    "Python `requests` is a  module that allows you to send HTTP requests using Python. The HTTP request returns a 'Response Object' with all the response data (content, encoding, status, etc).\n",
    "\n",
    "Once we have accessed the HTML content in a 'Response Object', we need to parse the data. One needs a parser which can create a nested/tree structure of the HTML data. There are many HTML parser libraries available but the most advanced one is `html5lib`.\n",
    "\n",
    "Python `BeautifulSoup` is a  library (from https://www.crummy.com/software/BeautifulSoup/) for pulling data out of `HTML` and `XML` files. This document covers `BeautifulSoup` version 4 that works with Python 3.\n",
    "\n",
    "#### Installing `BeautifulSoup`, `html5lib` and `requests`\n",
    "\n",
    "To install these libraries, use PIP.\n",
    "\n",
    "```python\n",
    "    python -m pip install requests\n",
    "    pip install html5lib\n",
    "    pip install beautifulsoup4\n",
    "```\n",
    "\n",
    "#### Steps involved in web scraping:\n",
    "\n",
    "1. Send an HTTP request to the URL of the webpage you want to access. The server responds to the request by returning the HTML content of the webpage. For this task, we will use `requests`\n",
    "\n",
    "2. Once we have accessed the HTML content, we are left with the task of parsing the data. Since most of the HTML data is nested, we cannot extract data simply through string processing. One needs a parser which can create a nested/tree structure of the HTML data. For this task, we will use `html5lib`\n",
    "\n",
    "3. Now, we need to navigate and search the parsed tree that we created, i.e. tree traversal. For this task, we will use `BeautifulSoup`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368b6a2c",
   "metadata": {},
   "source": [
    "\n",
    "#### **Step 1.) - Accessing the HTML content from a webpage**\n",
    "\n",
    "Import the requests library. Then, specify the URL of the webpage you want to scrape.\n",
    "Send a HTTP request to the specified URL and save the response from the server in a response object called `response`.\n",
    "Now, as print `response.content` to get the raw HTML content of the webpage. It is of ‘string’ type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33416a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import requests\n",
    "\n",
    "# request the URL of the webpage you want to access\n",
    "URL = \"http://www.dr-chuck.com/page1.htm\"\n",
    "response = requests.get(URL)\n",
    "\n",
    "#  print 'response.content' to get the raw HTML content of the webpage. It's of ‘string’ type\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e83c9ca",
   "metadata": {},
   "source": [
    "\n",
    "#### **Step 2.) - Parsing the HTML content**\n",
    "\n",
    "A really nice thing about the BeautifulSoup library is that it is built on the top of the HTML parsing libraries like html5lib, lxml, html.parser, etc. So the BeautifulSoup object and specifying the parser library can be done at the same time.\n",
    "\n",
    "We create a BeautifulSoup object by passing two arguments:\n",
    "- **response.content** : It is the raw HTML content.\n",
    "- **html5lib** : Specifying the HTML parser we want to use.\n",
    "\n",
    "Printing `soup.prettify()` gives the visual representation of the parse tree created from the raw HTML content. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399a5890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# request the URL of the webpage you want to access\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as soup\n",
    "\n",
    "URL = \"http://www.values.com/inspirational-quotes\"\n",
    "response = requests.get(URL)\n",
    "\n",
    "# parse the response into a readable form\n",
    "results = soup(response.content, 'html5lib')\n",
    "print(results.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2aa5d9",
   "metadata": {},
   "source": [
    "#### **Step 3.) - Searching and navigating through the parse tree**\n",
    "\n",
    "Now, we would like to extract some useful data from the HTML content. The soup object contains all the data in the nested structure which could be programmatically extracted. \n",
    "\n",
    "In our example, we are scraping a webpage consisting of some quotes. So, we would like to create a program to save those quotes (and all relevant information about them). \n",
    "\n",
    "Here below is code to do that ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bb7df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Program to scrape website and save quotes from website\n",
    "import requests\n",
    "from bs4 import BeautifulSoup  as bsoup\n",
    "import csv\n",
    "\n",
    "# request the URL of the webpage you want to access\n",
    "URL = \"http://www.values.com/inspirational-quotes\"\n",
    "response = requests.get(URL)\n",
    "\n",
    "# parse the response into a readable form\n",
    "results = bsoup(response.content, 'html5lib')\n",
    "# print(soup.prettify())\n",
    "\n",
    "quotes=[] # a list to store quotes\n",
    "\n",
    "# search the response for the HTML container that holds the quotes\n",
    "table = results.find('div', attrs = {'id':'all_quotes'})\n",
    "# print(table)\n",
    "\n",
    "# iterate the able rows to find each quote info\n",
    "for row in table.findAll('div'):\n",
    "    quote = {}  # create a dictionary for each quote\n",
    "    quote['url'] = \"https:/\" + row.a['href']\n",
    "    quote['lines'] = row.img['alt'].split(\" #\")[0]\n",
    "    quote['theme'] = row.h5.a.text\n",
    "    quote['img'] = row.img['src']\n",
    "    quotes.append(quote)    # attache each quote to the list\n",
    "\n",
    "print(quotes) \n",
    "\n",
    "# save the quotes list of dictionaries into a CSV file\n",
    "filename = 'inspirational_quotes.csv'\n",
    "with open(filename, 'w', newline='') as f:\n",
    "\tw = csv.DictWriter(f,['theme','url','img','lines'])\n",
    "\tw.writeheader()\n",
    "\tfor quote in quotes:\n",
    "\t\tw.writerow(quote)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128159de",
   "metadata": {},
   "source": [
    "##### **Lets analyze the above code**\n",
    "\n",
    "- First search through the HTML content of the webpage; print it using `soup.prettify()` method and try to find a pattern or a way to navigate to the quotes.\n",
    "\n",
    "- The quotes are inside a `div` container whose `id` is ‘all_quotes’. So, we find that div element by using the `find()` method :\n",
    "\n",
    "            table = soup.find('div', attrs = {'id':'all_quotes'}) \n",
    "\n",
    "- The first argument is the HTML `div` tag we want; the second argument is a dictionary type element to specify the additional attributes associated with that tag. \n",
    "\n",
    "- The `find()` method returns the first matching element. We can try to print `table.prettify()` to get a sense of what this piece of code does.\n",
    "\n",
    "- Now, in the table element, one can notice that each quote is inside a div container whose class is quote. So, we iterate through each div container with that class.\n",
    "\n",
    "- Finally, we use the `findAll()` method which is similar to the `find()` method in terms of arguments but it returns a list of all matching elements. Each quote is now iterated using a variable called row.\n",
    "\n",
    "- Using the row variable, we find info snippets on each quote to populate a quote dictionary, which is added to the quotes list.\n",
    "\n",
    "- Finally, save the quotes list of dictionaries into a CSV file.\n",
    "\n",
    ">Read all the documentation on using the BeautifulSoup library in the [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b8fc58",
   "metadata": {},
   "source": [
    "### A Note on Scraping Dynamic Websites\n",
    "\n",
    "In this section we learned how to scrape a static website. \n",
    "\n",
    "Static sites are straightforward to work with because the server sends you an HTML page that already contains all the page information in the response. You can parse that HTML response and immediately begin to pick out the relevant data.\n",
    "\n",
    "On the other hand, with a dynamic website, the server might not send back any HTML at all. Instead, you could receive JavaScript code as a response. This code will look completely different from what you saw when you inspected the page with your browser’s developer tools.\n",
    "\n",
    "Many modern web applications are designed to provide their functionality in collaboration with the clients’ browsers. Instead of sending HTML pages, these apps send JavaScript code that instructs your browser to create the desired HTML. \n",
    "\n",
    "Web apps deliver dynamic content in this way to offload work from the server to the clients’ machines as well as to avoid page reloads and improve the overall user experience.\n",
    "\n",
    "When we use `requests`, we only receive what the server sends back. In the case of a dynamic website, you’ll end up with some JavaScript code instead of HTML. \n",
    "\n",
    "The only way to go from the JavaScript code you received to the content that you’re interested in is to execute the code, just like your browser does. The `requests` library can’t do that for you, but there are other solutions that can.\n",
    "\n",
    "For example, `requests-html` is a project created by the author of the `requests` library that allows you to render JavaScript using syntax that’s similar to the syntax in requests. It also includes capabilities for parsing the data by using Beautiful Soup under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0731d23",
   "metadata": {},
   "source": [
    "### Closing note\n",
    "\n",
    "In today’s highly-connected and instantaneous world, we have access to a massive amount of information at our fingertips.\n",
    "\n",
    "Inter-Connected Media was a huge step forward in that it enabled everyone to be a part of the conversation. On the other hand, algorithms and the sheer amount of content to sift through, has created a lot of downsides as well.\n",
    "\n",
    "Between 2015 and 2025, the amount of data captured, created, and replicated globally will increase by 1,600%.\n",
    "\n",
    "Read this article on [The Evolution of Media: Visualizing a Data-Driven Future](https://www.visualcapitalist.com/evolution-of-media-data-future/) and specifically look at the info-graphic that goes with the article.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
