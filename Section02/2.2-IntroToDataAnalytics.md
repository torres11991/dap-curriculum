# Introduction to DAP

## Objectives of this section:-

- Why DAP
- Who are data Analysts
- Why Python
- The Data Science Life Cycle (DSLC)
- Crisp-DM
- Data Wrangling
- Our Capstone Project

<br>

![Intro to DAP](images/IntroToDAP.png)

<br>

---

<br>

## Data Analytics and Python skills are HOT right now

Because of the phenomenal growth in data gathering and data interrogation, companies are looking for people who can maintain their data and analyze it.

> **Definition**: - The role of a Data Analyst is to extract and catalogue data, ...
>
> - so that organizations can pinpoint and evaluate relationships, patterns and trends ...
> - to glean insights and draw conclusions based on the data, and ...
> - then use these to make informed decisions.

<br>

## Why Data Analysis?

1. Data analytics is important to understand trends and patterns from the massive amounts of data that are being collected.

1. Data analytics enables organizations to uncover patterns and extract valuable insights from raw data.

1. Analytics provides insights to help optimize business performance, forecast future results, understand trends, and reduce costs.

1. Analytics helps companies understand their customers better, produce relevant content, strategize ad campaigns, develop meaningful products, and ultimately boost business performance.

<br>

### **The four main groups / categories of data analysis are**:-

- **Descriptive Analysis** -- the process of using current and historical data to identify trends and relationships; Often called the simplest form of data analysis because it describes trends and relationships but doesn't dig deeper.

- **Diagnostic Analysis** -- a form of advanced analytics that examines data or content to answer the question, “Why did it happen?” It is characterized by techniques such as drill-down, data discovery, data mining and correlations.

- **Predictive Analysis** -- a branch of advanced analytics that makes predictions about future outcomes using historical data combined with statistical modeling, data mining techniques and machine learning. Companies employ predictive analytics to find patterns in data to identify risks and opportunities.

- **Prescriptive Analysis** -- the process of using data to determine an optimal course of action. By considering all relevant factors, this type of analysis yields recommendations for next steps, answering the question "What should we do?" Because of this, prescriptive analytics is a valuable tool for data-driven decision-making.

<br>

### **The 5 V’s of Data Analytics**

Data is often described by the five V’s of Data Analytics: -

1. **Volume:** The size of data that is managed and analyze
1. **Value:** Value comes from comprehending data and pattern recognition that direct to more compelling operations
1. **Variety:** The diversity and range of various data types
1. **Velocity:** The speed at which businesses obtain, store and organize data
1. **Veracity:** The accuracy of data and data assets, which defines executive-level confidence

<br>

### Data Analytics has become imperative in fields like

- Transportation, Logistics, and Delivery (Supply Chain Management)
- Manufacturing, and Manufacturing processes
- Web Search Optimization (SEO), and Digital Marketing / Advertisement
- Insurance and Assessment
- Fraud and Risk Detection
- Security and Safety Industry
- Military
- Education
- Healthcare
- Banking and Finance
- Energy Industry
- Utility, and Services provisioning
- Travel and Tourism
- Communication and Media (Nielsen ratings)
- Sports, and Entertainment Industries

<br>

---

<br>

## Who are data Analysts?

The average Data Analyst is likely a natural problem-solver: Perceptive, analytical, and detail-oriented.

The average Data Analyst tends to be confident and insightful, enjoying deep discussion to understand a particular issue.

While data analysts should have a foundational knowledge of statistics and mathematics, much of their work can be done without complex mathematics.

“Data analysis work" varies depending on the type of data that you're working with like sales, social media, inventory, demographics, etc.

<br>

### **Some Job titles / roles for Analysts**:-

- Business Intelligence Analyst
- Data Analyst / Scientist / Engineer
- Quantitative Analyst
- Data Analytics Consultant
- Operations Analyst
- Marketing Analyst
- Database analyst

<br>

### **Skills used in Data Analysis**

Skills used in the field of data Analytics include topics like R, Python, Database Management, SQL, Excel, Data Wrangling, Data Visualization, BI, Tableau, Statistical Analysis, Machine Learning, AI, etc.

<br>

---

<br>

## Why Python

Python is a highly versatile, general-purpose, object-oriented programming language that can be used for small and complex tasks such as: backend development, software development, data science, and writing system scripts (automation) across many different industries.

Python has become a staple in Data Science, allowing data analysts and other professionals to use the language to --

- conduct complex statistical calculations,
- create data visualizations,
- build machine learning algorithms,
- manipulate and analyze data,
- and complete other data-related tasks.

Python Developers are in high demand because the language is so popular and widely used as a solution in so many different areas of data science and machine learning.

Note however, to become a good analyst, it is not enough to just learn the Python language itself; You'll need to learn about the core Python libraries, and maybe also a Python Framework.

Further more, as an Analyst, you'll need to master some additional skills like ...

- Versioning Management using something like Git,
- be comfortable with different database technologies,
- brush up on statistical processes,
- gain a basic understanding of front-end technologies (HTML5, CSS3, JavaScript),
- become a user of Project Management processes like Agile,
- and develop excellent communication skills.

<br>

### Python Learning Materials

- The Python we'll be able to teach is a basic overview of the Python language - enough to enable you to code simple solutions
- Our material is based on "Python For Everyone -- PY4E"
- The PY4E program was created by Dr. Charles Severance (a.k.a. Dr. Chuck)
- He is a Clinical Professor at the University of Michigan School of Information Technology
- We encourage you to use these materials as we cover the different sections of Python
- All materials are available of FREE at ...  [https://www.py4e.com/](https://www.py4e.com/)

<br>

---

<br>

## What Is The Data Analytics Lifecycle?

Data undergoes various change processes throughout its life, during its creation, testing, processing, consumption, and reuse.

The Data Analytics Lifecycle maps out these stages for professionals working on data analytics projects.

These phases are often arranged in a circular structure that depicts the Data Analytics Life-circle / cycle.

The Lifecycle is an iterative, step-by-step model for arranging the actions and events involved in gathering, processing, analyzing, and reusing data.

<br>

![Data Analytics Lifecycle](images/Data-Science-LifeCycle.png)

<br>

1. Business Understanding  -- What does the business need?

1. Data Understanding -- What data do we have / need? Is it clean?

1. Data preparation -- How do we organize the data for modeling?

1. Modeling -- What modeling techniques should we apply?

1. Evaluation -- Which model best meets the business objectives?

1. Deployment -- How do stakeholders access the results?

<br>

### What is Crisp-DM?

**Crisp-DM** stands for the "CRoss Industry Standard Process for Data Mining"

It is a process model with six phases that naturally describes the data science life cycle.

<br>

![Crisp-DM](images/Crisp-DM.png)

<br>

The CRoss Industry Standard Process for Data Mining (CRISP-DM) is a model that serves as the base for a data science process.

Published in 1999 to standardize data mining processes across industries, it is the most common methodology for data mining, analytics, and data science projects.

Data science teams that combine a loose implementation of CRISP-DM with overarching team-based **Agile project management** approaches will likely see the best results.

Read all about the CRISP-DM model here in this article [What is CRISP DM?](https://www.datascience-pm.com/crisp-dm-2/)

<br>

---

<br>

## Data Wrangling

The process of cleaning and unifying messy and complex data sets for easy access and analysis.

Data professionals spend as much as 80% of their time in data wrangling -- organizing and processing data.

<br>

### Examples of Data Wrangling

- Joining together multiple data sets into one
- Finding gaps in data and filling/deleting them
- Getting rid of data that is unnecessary
- Identifying extreme outliers and either explaining them or getting rid of them

<br>

### Goals of Efficient Data Wrangling

- Show "deeper intelligence" by gathering data from several different sources
- Provide accurate, actionable data to clients, on time
- Reduces time spent collecting and organizing raw data
- Allow data scientists and analysts to focus on the analysis
- Provide intelligence for better decision-making by leaders

<br>

### Six Data Wrangling Steps

<br>

![The 6 Data Wrangling Steps](images/DataWranglingSteps.png)

<br>

#### **Step #1 – Discovery**

- Find data that addresses your question
- Become familiar with your data so that you know how you will end up using it
- Identify trends, patterns and some data cells / sections that might cause issues in analysis.

#### **Step #2 – Structuring**

- Take your raw data and transform it to what you can work with
- Unstructured data is often text-heavy and contains things such as Dates, Numbers, ID codes, etc.
- Example: - When using info scrapped from a website, you might parse HTML code, pull out what you need, and discard the rest.

#### **Step #3 – Cleaning**

- Removes outliers that can potentially skew your results when analyzing the data
- Changes any null values and standardizes the data format to improve quality and consistency
- Identifies duplicate values, standardizes systems of measurements, fixes structural errors and typos, and validates the data to make it easier to handle

#### **Step #4 – Enriching**

- Deciding if you need to add to the data by combining raw data with additional data from other sources.
- Example: - Combining two or more databases of customer information to fill in gaps in the data
- Enriching the data is an optional step that you only need to take if your current data doesn’t meet your requirements.

#### **Step #5 – Validating**

- Making sure that the data that you have is of the quality necessary to complete your project.
- The rules of data validation require repetitive programming processes that help to verify the – Quality, Consistency, Accuracy, Security, and Authenticity of data

#### **Step #6 – Publishing**

- Creating your analysis and presenting it to the public.
- You can deposit the data into a new architecture or database.
- We will display our data story using Tableau.

<br>

### Data Wrangling vs. Data Cleaning

What’s the Difference between Data Wrangling and Data Cleaning?

**DATA WRANGLING** changes the data’s format by making the raw data into something more useable, and preparing the data’s structure for modeling

**DATA CLEANING** removes data that will not help in analysis because it contains errors or misinformation. It enhances the data’s accuracy, integrity, and quality..

<br>

---

<br>

## The Outcomes of this DAP BootCamp: - Your Capstone Project

The cohort will conclude with a live demonstration, using the tools and processes we'll cover in this cohort.

You will get to showcase the knowledge you've gained, and demonstrate what you've learned to do in this 12-week period.

<br>

### Requirements for your Capstone project:-

The capstone exercise is intended to apply all of the knowledge and skills you've gained over the course in one assignment.

You will take your capstone project through the data Analytics Lifecycle, to show-case your achievements. You are required to use (1) Python, (2) Excel or SQL, and (3) Tableau in this process.

#### **Analysis of Data**

- You'll choose any publicly Internet-available dataset(s)
- Collect and wrangle that data in a programmatic fashion
- Optionally illustrate a particularly step in the data wrangling / analytics process
- Generate some charts, graphs or reports to visualize some insightful aspects from the data
- And finally present your Capstone project to tell the Data Story

Each student should be able to walk through their code, providing a straightforward explanation of the various use cases, and specifically point out initial inputs and expected results, any challenges, and derived solutions.

> Be creative with your presentation as your audience will not want to look at a record level, tabular file, or lines and lines of code.
>
> **Hint** – some colorful programmatic visualization will help to drive your points home by keeping your audience engaged.

<br>

### Requirements for your Demo Day Presentation:-

- 15 – 20 minutes in length
- You need to come up with a topic
- Create a question with that topic
- Clean and analyze data to get an answer
- Create and present your story

<br>

The full explanation of the Capstone and Demo Day requirements, can be found in **Section00 of the curriculum**, in the `"Savvy_DAP_CapstoneGuidelines_2202-10-18.pdf"` document. Feel free to download this for reference.

<br>

![Data Stories](images/DataStories.png)
